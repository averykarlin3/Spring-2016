\documentclass[11 pt, twoside]{article}
\usepackage{textcomp}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{color}
%\usepackage{indentfirst} %Comment out for no first paragraph indent
\usepackage[parfill]{parskip}
\usepackage{setspace}
\usepackage{tikz}
\usetikzlibrary{automata, topaths}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

%\usepackage{outlines}

\usepackage{fancyhdr}
\pagestyle{fancy}
\cfoot{\hyperlink{content}{\thepage}}
\lhead{}
\chead{}
\rfoot{}
\lfoot{}
\rhead{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}


\usepackage{hyperref}
\hypersetup {
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}

%\newenvironment{outline*}
%{
%	\begin{outline}[enumerate]
%	\setlength{\itemsep}{0pt}
%	\setlength{\parsep}{0pt}
%	}
%	{\end{outline}
%}

\newcommand{\realn}{\mathbb{R}^n}
\newcommand{\pfrac}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\grad}{\vec{\nabla}}

\begin{document}

\title{Real Analysis of Differential Equations}
\author{Avery Karlin}
\date{Spring 2016}
%\newcommand{\textbook}{}
\newcommand{\teacher}{Stern}

\maketitle
\newpage
\hypertarget{content}{\tableofcontents}
\vspace{11pt}
\noindent
%\underline{Primary Textbook}: \textbook\\
\underline{Teacher}: \teacher
\newpage

\section{Introduction}

\subsection{Definitions}
\textbf{Rigorous mathematics} are based on the idea of evaluating facts purely based on definitions and logical axioms of set theory, rather than proofs of convincing, such that something cannot be assumed to be valid because it appears so, but rather only axioms, deemed a good starting point of the mathematical, abstract world.

\textbf{Differential equations} are relations between one or more unknown functions an a finate amount of their derivatives, along with a certain number of independent variables. Most are not solvable, and do not have approximations for more than a minute region of the function.

\textbf{Linear differential equations} are defined as equations with algebraic functions as the multiplier of each function derviative and each derivative with a degree/power of 1, such that all linear equations are able to be solved.

\textbf{First order differential equations} means that the highest derivative in the equation is the first derivative, and is extended as such.

\textbf{Coupled systems} are those defined as requiring being solved together, such as $$X' = aX + bY, Y' = cX + bY,$$ rather than equations containing only their own function, which would be uncoupled.

\textbf{Ordinary Differential Equations} are those where all unknown functions depend on the same, single independent variable, while \textbf{Partial Differential Equations} are those where the functions have multiple independent variables. In addition though, vector function differential equations can fall into either catagory, action more as a system of differential equations, rather than a single one. \textbf{Standard form} is written in the format of $G(t, x, x', \dots, x^{(n)}) = 0$, where x = x(t), the unknown function.

Finally, in general, either an initial condition is set allowing it to be solved for a single solution, or a family of solution must be considered as the answer.

\textbf{Vector fields} are used to determine the trajectory of a function based on the initial condition, such that in some region D, for every point $\vec{x}$, we attach the vector $\vec{F}(\vec{x})$ to the point, represented as a vector from $\vec{x}$.

\textbf{Explicit solutions} are defined as those equal to x, while \textbf{Implicit solutions} are those with some algebraic relationship to x in terms of the parameter.

\subsection{Example}

For some identically restricted, coupled functions, $$R' = aR + bJ, J' = bR + aJ,$$ where R(0) = $R_0$ and J(0) = $J_0$, where $a < 0, b > 0$, if $|a| > |b|$, we can graph the phase plane of R on the x-axis, J on the y-axis, such that for all possible functions, it moves toward the stable node (0, 0). These stable nodes don't need to be a point, but rather can be a curve of some sort. If $|a| < |b|$, if the initial points are above R = -J, it moves in a parabolic fashion until asymptotic to R = J on the positive side. If below R = -J, it moves similarly, except asymptotic to R = J on the negative side. Finally, if $|a| = |b|$, the function moves infinitely in a circle around the origin, with the radius determined by the initial point.

This is a form of phase plane analysis, finding the long term behavior, rather than a particular solution, used in cases where the specific solution cannot be found. Algorithms can also be used to approximate the solution at specific points.

\section{Mathematical Concepts}
\subsection{Sup-norms}
%Add Sup-Norm
\section{Basic Existence and Uniqueness Theorems}
\subsection{Flow Theorem}
\subsubsection{Theorem} 
Let $\vec{F}(\vec{x}) = (F_1(\vec{x}), F_2(\vec{x}), \dots, F_n(\vec{x}))$ be a vector field (the assignment of a vector to each point within some subset of space) defined on some closed, bounded region D in $\realn$. Also assume $\vec{F}$ is $C^1$ and let $\vec{p}$ be a specific point interior to D. Then $\exists$ a function $\vec{\sigma}(t)$ from some time interval t, $(-\epsilon, \epsilon)$ with $\epsilon > 0$ into D, such that $\vec{\sigma}(0) = \vec{p}$ and $\vec{\sigma}'(t) = \vec{F}(\vec{\sigma}(t)).$

Thus, for any point on the interval t, the velocity of $\vec{\sigma}$ is the vector field, such that we call $\vec{\sigma}$ the flow for $\vec{F}$ on that interval, starting at $\vec{p}$, which we call the initial condition.

The flow is unique, such that for any two flows for $\vec{F}$ starting at the same initial condition, they must agree whenever both are defined.

This can easily be extended to higher derivatives, such that it is written as a system of two differential equations, each going one derivative, though an initial condition is needed for both of the systems.

\subsubsection{Application}
For some nth-order differential equation, $f(t)x^{(n)} = F(t, x, x', x'', \dots, x^{(n-1)})$, written in \textbf{singular standard form}, assuming there is no singularity (value at which $f(t_0)$ = 0, such that the equation becomes algebraic at that point), and f(t) is continuous, such that it is of constant sign by the Intermediate Value Theorem on the interval, we divide by f(t) to put the equation explicitly equal to the nth derivative term, called the \textbf{regular standard form case}. It must also be an initial value problem, such that initial values for each derivative and the function, x(t), itself, are given for some initial time, $t_0$. It should be noted that t and x can be vectors/vector-valued functions as well, and this would still be valid.

The process of creating a system of lower order equations from a higher order differential equation, given initial values, is called an initial value problem (IVP).

\underline{Theorem:} There is a unique solution, $\sigma = x(t)$ defined in some time interval $(t_0 - \epsilon, t_0 + \epsilon)$, where $\epsilon > 0$.

\underline{Proof:}
Let $x_0 = t, x_1 = x_1(t) = x(t), x_2 = x(t), \dots, x_n = x^{(n-1)}(t)$, such that $x_0' = 1, x_1' = x_2, x_2' = x_3, \dots, x_n' = x^{(n)}$. It should also be noted that $x_0' = x_1 = F_0(x_0, x_1, \dots, x_n), x_1' = F_1(x_0, x_1, \dots, x_n)$, and so forth, removing the time dependence, such that each is purely in terms of $\vec{x} = (x_0, x_1, \dots, x_n)$. 

Thus, the flow theorem is able to apply, assuming that the $F_d(\vec{x})$ functions are all $C^1$. Since each of the functions are either constant or continuous, with the exception of $x_n'$, we must show that F is $C^1$ for there to be a unique solution.

\subsubsection{Proof}
We prove this theorem in 2 dimensions, but the proof will be general enough for
n-dimensions.

Let us start with vector field $\vec{F}(x, y) = (A(x, y), B(x, y))$ where $A$
and $B$ are defined on a closed, bounded region $D$ and are $C^1$. The systm
differential equation we are trying to solve is:
\[
\begin{cases}
x' &= A(x, y)\\
y' &= B(x, y)
\end{cases}
\]

where $x = x(t)$ and $y = y(t)$. The system can also be expressed as a single
vector function $\vec{x}' = \vec{F}(\vec{x})$. Now we need the initial
condition, let $\vec{x}(0) = \vec{p}$.

First let us take the difference between two values of $A$:
$|A(x_1, y_1) - A(x_2, y_2)| = |A(x_1, y_1) - A(x_1, y_2) + A(x_1, y_2) - A(x_2, y_2)| \leq |A(x_1, y_1) - A(x_1, y_2)| + |A(x_1, y_2) - A(x_2, y_2)| \leq |\pfrac{A}{y} (x_1, y^*)(y_1 - y_2)| + |\pfrac{A}{x} (x^*, y_2)(x_1 - x_2)|$

where $x^*$ is some $x$ between $x_1$ and $x_2$, and $y^*$ is some $y$ between
$y_1$ and $y_2$.

%Since the derivatives of $A$ and $B$ are continuous functions defined over closed and bounded intervals, take $K =
%\max\{\left|\pfrac{A}{x}\right|,\left|\pfrac{A}{y}\right|,\left|\pfrac{B}{x}\right|,\left|\pfrac{B}{y}\right|\}$
%over $D$, so we get:
%\begin{align*}
%\left|\pfrac{A}{y} (x_1, y^*)(y_1 - y_2)\right| + \left|\pfrac{A}{x}
%(x^*, y_2)(x_1 - x_2)\right| &= \left|\pfrac{A}{y} (x_1,
%y^*)\right|\left|y_1 - y_2\right| + \left|\pfrac{A}{x} (x^*, y_2)\right|
%\left|x_1 - x_2\right|\\
%&\leq K(\left|x_1 - x_2\right| + \left|y_1 - y_2\right|)
%\end{align*}

%By the same logic, $\left|B(x_1, y_1) - B(x_2, y_2)\right| \leq K(|x_1 - x_2| + |y_1 - y_2|)$. These two inequalities are known as the \textbf{Lipschitz Condition}.

Since $A$ and $B$ are continuous themselves on D, by the EVT we can find an upper bound $M$ for $|A|$ and $|B|$ on $D$:

\[
M = \max(\max_{(x,y) \in D} |A(x, y)|, \max_{(x, y) \in D} |B(x, y)|)
\]

The initial point $(p, q)$ is assumed to be on the \textit{interior} of $D$. So we can surround the point $p$ with a rectangle with corners defined by $(p - r, q - s), (p+r, q - s), (p+r, q+s), (p-r, q+s)$ for some $r, s > 0$ such that the rectangle lies within $D$. Now draw two lines through $(p, q)$, one with slope $M$ and one with slope $-M$.

%\begin{figure}[ht]
%\centering
%\begin{tikzpicture}
%\begin{axis} [
%scale only axis,
%grid=major,
%axis lines=middle,
%inner axis line style={->},
%xmin=-1, xmax=6,
%ymin=-1, ymax=4,
%xlabel = t,
%ylabel = x,
%ticks=none,
%grid=none,
%]
%\addplot[mark=none] coordinates {(1,1) (1,3) (5,3) (5,1) (1,1)};
%\addplot[mark=*] coordinates {(3,2)} node[anchor=south] {$(t_0, p)$};
%\addplot[mark=none] coordinates {(4,0) (2,4)} node[anchor=north] {$-M$};
%\addplot[mark=none] coordinates {(2,0) (4,4)} node[anchor=north] {$M$};
%\addplot[mark=none, dashed] coordinates {(3.5, 0) (3.5, 3)};
%\addplot[mark=none, dashed] coordinates {(2.5, 0) (2.5, 3)};
%\addplot[mark=none] coordinates {(3.25, 0.75)} node {$h$};
%\addplot[mark=none] coordinates {(2.75, 0.75)} node {$h$};
%\addplot[->] coordinates {(3.05, 0.5) (3.5, 0.5)};
%\addplot[->] coordinates {(2.95, 0.5) (2.5, 0.5)};
%\end{axis}
%\end{tikzpicture}
%\end{figure}

Define $h := \min(r, \frac{s}{M}) > 0$

\subsection{Uniqueness for $C^1$ IVPs}
\subsubsection{Theorem}
First order IVPs are expressed in standard form as x' = F(t, x), $x(t_0) = x_0$, where it is assumed that F is a $C^1$ function on a domain of a rectangle centered around the initial point.

Thus, the theorem states that if $\phi(t), \omega(t)$ are solutions of the IVP defined on intervals $I_\delta = (t_0 - \delta, t_0 + \delta)$ and $I_\epsilon = (t_0 - \epsilon, t_0 + \epsilon)$, where $\delta, \epsilon > 0$. Then, $\phi(t) \equiv$ (equivilance at all points operator, rather than in a particular case) $\omega(t)$ for all $t \in I_n = (t_0 - n, t_0 + n)$, where n > 0.

On the other hand, it must be noted that non-$C^1$ functions cannot be assumed to have a unique solution.

\subsubsection{Proof}
%Add Proof Here
\subsection{Existence for $C^1$ IVPs}
\subsubsection{Theorem}
If we have some first order IVP, F(t, x(t)), assumed to be $C^1$ within some rectangle R, we can rewrite it as an integral equation, $x(t) = x_0 + \int^t_{t_0} F(\tau, x(\tau))d\tau$, after which we use the \textbf{Picard Method} to prove that a solution exists.

\subsubsection{Picard Method}
We define a sequence of functions, such that $x_{n+1}(t) = x_0 + \int_{t_0}^t F(\tau, x_n(\tau))d\tau$, but we need a base case. Since the singular solution is known to exist ($x_0(t) \equiv x_0$), we use the function $x_0$ as the base case. 



\section{First Order Scalar Ordinary Differential Equations}
\subsection{Separable Linear Equations}
\textbf{Multiplicatively seperable functions} are those of the format $F(t, x) = f(t)g(x)$, such that \textbf{Seperable ODEs} are those of the form, $x'(t) = f(t)g(x)$.

For any function in regular standard form, a vector field can be drawn with t on the x-axis and x(t) on the y-axis, such that there is a grid of vectors, with the slope of the unknown function drawn at each point, allowing the trajectory from each initial point to be drawn.

Thus, for a separable differential equation, it can be drawn at any point on the x-axis, such that g(x) is the single slope function at that t.

First, we can divide by g(x), such that $f(t) = \frac{x'(t)}{g(x(t))}$, assuming that g(x(t)) is non-zero within some region, I. We thenintegrate with respect to some variable, $\tau$, such that $\int^t_a \frac{x'(\tau)}{g(x(\tau))}d\tau = \int^t_a f(\tau)d\tau$, leaving it in terms of the integral in case it is some nonstandard function, such that it cannot be differentiated precisely. Then, we let $u = x(\tau)$, such that $\int^{x(t)}_{x(a)} \frac{du}{g(u} = \int^t_a f(\tau)d\tau = F(t)$. In addition, $\int^v_{x(a)} \frac{1}{g(u)}du = G(v)$. Then, F'(t) = f(t) and G(x(t)) = F(t) + c by the Fundemental Theorem of Calculus, where $c \in \mathbb{R}$ (some constant). Thus, x(t) = $G^{-1}(F(t) + c)$.


\section{Equation Techniques}
\subsection{First-Order Equations with an Isolated and Missing Variable}

\subsubsection*{Simple Forms}
For some equation, x' = f(t), it can be solved simply by taking the integral of both sides, as the simplest form of a differential equation. Similarly, for some equation x = f(t) or t = f(x), it  would not be a differential equation, providing the solution. On the other hand, all equations of the general form of a missing and isolated variable can be solved simply.

For some equation of the form x' = f(x), it is a seperable equation, such that it is trivial to solve by breaking up, with the singular solution of where f(x) = 0, forming a constant function. It is considered the singular case due to not fitting within the general solution for any constant of integration.

\subsubsection*{t = f(x') and x = f(x')}
For some equation, t = f(x'), where f is $C^1$ on its domain, if it is one-to-one, such that there is an inverse, it can be rewritten such that x' = g(t), where g is the inverse of f. Since g would also be $C^1$ on the interval, with a simple relationship between g' and f', such that it exists over the interval and f'(g(t)) $\neq$ 0 throughout the relevant interval (since $g'(t) = \frac{1}{f'(g(t))}$, such that otherwise, it wouldn't be $C^1$ over the interval), it is trivial.

If it does not have an inverse of this, it must be solved by different means, such that p = x', such that t = f(p). Thus, if x can be put in terms of p, parametric equations can be found for x and t over the parametric interval of p, such that it varies throughout the common domain of the two parametric functions, though it also only exists on the domain of the derivative itself, such that it follows the problem itself. It is quickly derived that $\frac{dx}{dp} = \frac{dx}{dt} \cdot \frac{dt}{dp} = p \cdot \frac{d[f(p)]}{dp} = pf'(p)$ by the chain rule, such that it can be integrated to get $x = \int pf'(p)dp + C$ as the second parametric equation. 

Similarly, for x = f(x'), it can by derived by the same means, such that $\frac{dt}{dp} = \frac{f'(p)}{p}$ by the chain rule, giving the second parametric equation of $t = \int \frac{f'(p)}{p}dp$ for the solution. The family of solution curves for both can then be found based on the constant of integration changing within the bounds created by the variable limits, through integration of the function, such that the integration constant is from the lower bound of the integral, within the function domain.

\end{document}
